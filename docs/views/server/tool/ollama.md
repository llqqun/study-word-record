# ollama

[ollama](https://ollama.com) is a local LLM runner.

[github](https://github.com/ollama/ollama)

ollamay一个开源的大型语言模型服务工具


## 可用命令
0. 创建自定义模型
```
ollama create
```
1. 启动ollama服务
```
ollama serve
```
2. 安装ollama模型
```
ollama pull [modelName]
```
3. 启动ollama模型  
没有模型的情况下，默认会主动去下载模型
```
ollama run [modelName]
```
4. 删除
```
ollama rm [modelName]
```
5. 停止运行的模型
```
ollama stop [modelName]
```
6. 列出所有模型
```
ollama list
```

## 根据Modelfile创建自定义模型

.Modelfile文件

```Modelfile
FROM llama3.2

SYSTEM """
你是一个阅读助手，帮助用户理解和分析文本内容。你可以总结文本、提取关键信息、回答问题，并提供相关的背景知识。
"""

# 设置 Ollama 如何运行模型的参数。
PARAMETER num_ctx 4096

# 要发送给模型的完整提示模板。
TEMPLATE

# 定义适用于模型的 (Q)LoRA 适配器。
ADAPTER

#指定合法的许可证。
LICENSE

#指定消息历史记录。
MESSAGE

```

PARAMETER 参数

| 名称 | 说明 | 值类型 | 默认值 |
| --- | --- | --- | --- |
| mirostat | 启用米罗统计采样以控制困惑度。（默认值：0，0 表示禁用，1 表示米罗统计，2 表示米罗统计 2.0） | int | 0 |
| mirostat_eta | 影响算法对生成文本反馈做出响应的速度。较低的学习率会导致调整速度变慢，而较高的学习率会使算法响应更灵敏。 | float | 0.1 |
| mirostat_tau | 控制输出内容连贯性和多样性之间的平衡。较低的值会使生成的文本更聚焦、更连贯。 | float | 5.0 |
| num_ctx | 设置用于生成下一个词元的上下文窗口大小。 | int | 2048 |
| repeat_last_n | 设置模型为防止重复向前回看的距离。（默认值：64，0 表示禁用，-1 表示上下文数量） | int | 64 |
| repeat_penalty | 设置对重复内容惩罚的力度。较高的值（例如 1.5）会对重复进行更严厉的惩罚，而较低的值（例如 0.9）则更宽松。 | float | 1.1 |
| temperature | 模型的温度参数。提高温度会使模型的回答更具创造性。 | int | 0.7 |
| seed | 设置用于生成内容的随机数种子。将其设置为特定数字会使模型针对相同的提示生成相同的文本。 | int | 42 |
| stop | 设置要使用的停止序列。当遇到此模式时，大型语言模型（LLM）将停止生成文本并返回结果。可通过在模型文件中指定多个独立的停止参数来设置多个停止模式。 字符串类型 停止序列 “AI 助手：”
| tfs_z | 无尾采样用于降低输出中较不可能出现的词元的影响。较高的值（例如 2.0）会更大程度地降低这种影响，值为 1.0 则禁用此设置。 | float | 1 |
| num_predict | 生成文本时要预测的最大词元数量。（默认值：-1，即无限生成） | int | 42 |
| top_k | 降低生成无意义内容的概率。较高的值（例如 100）会给出更多样化的答案，而较低的值（例如 10）则更保守。（默认值：40） | int | 40 |
| top_p | 与前 k 采样共同起作用。较高的值（例如 0.95）会使生成的文本更具多样性，而较低的值（例如 0.5）会生成更聚焦、更保守的文本。 | float | 0.9 |
| min_p | 是核采样（top_p）的替代方案，旨在确保质量和多样性之间的平衡。参数 p 代表相对于最有可能出现的词元的概率而言，一个词元被考虑的最小概率。例如，当 p = 0.05 且最有可能出现的词元概率为 0.9 时，值小于 0.045 的对数几率（logits）会被过滤掉。 | float | 0.05 |

